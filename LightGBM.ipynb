{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef976fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137dad7",
   "metadata": {},
   "source": [
    "# **Train LightGBM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "train = pd.read_parquet('C:\\\\Users\\\\16122\\\\AMEX Kaggle Competition\\\\train_newnn_fe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a60406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in list(train.columns) if col not in ['customer_ID','S_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ad9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    if train[col].dtype == 'int64':\n",
    "        train[col] = train[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e49f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "for cat_col in cat_features:\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb111c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numerical features to two decimal places\n",
    "for col in list(train.columns):\n",
    "    if train[col].dtype == 'float32':\n",
    "        train[col] = train[col].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83544ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, features, cat_features):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.3,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 5,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    # CV and start training\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train[features], train['target'])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train['target'].iloc[trn_ind], train['target'].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 9500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric,)\n",
    "        # Save model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{42}_l2_5_fe{len(features)}_round2_9500_new.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train['target'], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to save predictions of each validation set\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train['target'], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{5}fold_seed{42}_l2_5_fe{len(features)}_round2_9500_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d966d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_and_evaluate(train, features, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03321da",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331ab7b",
   "metadata": {},
   "source": [
    "# **Feature selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42da5cd",
   "metadata": {},
   "source": [
    "I did not select features in my final submission. The main goal to compute feature importance is to see whether we can come out some new ideas to create new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 5\n",
    "features = [col for col in train.columns if col not in ['customer_ID', 'target']]\n",
    "imp_df = pd.DataFrame({'Feature': train[features].columns})\n",
    "ave = 0\n",
    "for i in range(fold):\n",
    "    model = joblib.load(f'lgbm_fold{i}_seed42_l2_5_fe1258_round2_9500_P2B4diff.pkl')\n",
    "    imp_df[f'fea_imp{i}'] = model.feature_importance()\n",
    "    ave += model.feature_importance()\n",
    "    imp_df[f'overall average'] = ave/fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80089b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values(by=['overall average'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df[imp_df['overall average'] > 0].Feature.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = imp_df[imp_df['overall average'] > 0].Feature.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943ffef",
   "metadata": {},
   "source": [
    "# **Prediction and submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test = pd.read_parquet(\"C:\\\\Users\\\\16122\\\\AMEX Kaggle Competition\\\\test_newnn_fe.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "for cat_col in cat_features:\n",
    "    encoder = LabelEncoder()\n",
    "    test[cat_col] = encoder.fit_transform(test[cat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numerical features to two decimal places\n",
    "for col in list(test.columns):\n",
    "    if test[col].dtype == 'float32':\n",
    "        test[col] = test[col].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b727a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Predict & Test\n",
    "# ====================================================\n",
    "def predict_test(test, features):\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    for fold in range(5):\n",
    "        # Predict the test set\n",
    "        print(f'=========== Fold {fold} is predicting ===========')\n",
    "        model = joblib.load(f'lgbm_fold{fold}_seed{42}_l2_5_fe{len(features)}_round2_selected_10500.pkl')\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / 5\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_{5}folds_seed{42}_l2_5_fe{len(features)}_round2_selected_10500.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cad57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1c1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
